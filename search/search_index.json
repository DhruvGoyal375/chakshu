{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Chakshu Developer Documentation","text":"<p>Chakshu is a backend application designed to fetch, process, and present information from web sources, primarily Wikipedia. This documentation provides developers with the necessary information to understand, maintain, and extend the Chakshu project.</p>"},{"location":"#purpose-of-this-documentation","title":"Purpose of this Documentation","text":"<p>This guide aims to:</p> <ul> <li>Explain the overall architecture of the Chakshu application.</li> <li>Detail the functionality of each module and Django app.</li> <li>Provide a comprehensive reference for the API endpoints.</li> <li>Guide developers on setting up the development environment.</li> <li>Offer insights into the core logic and external service integrations.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to the project, we recommend starting with the following sections:</p> <ol> <li>Project Overview: Understand the goals and high-level functionality of Chakshu.</li> <li>Setup Guide: Learn how to set up your development environment.</li> <li>Project Structure: Get familiar with the organization of the codebase.</li> </ol>"},{"location":"#navigating-the-documentation","title":"Navigating the Documentation","text":"<p>Use the navigation panel on the left (or top, depending on your screen size) to explore different sections of this documentation. Key sections include:</p> <ul> <li>API Reference: Detailed information about the available API endpoints, request parameters, and response formats.</li> <li>Modules: In-depth explanations of each Django app and its components.</li> <li>Configuration: Information on how to configure the application.</li> </ul> <p>We hope this documentation helps you get up to speed with Chakshu quickly!</p>"},{"location":"configuration/","title":"Configuration","text":"<p>This section outlines key configuration aspects of the Chakshu application, primarily managed through Django's <code>settings.py</code> file and potentially a custom <code>config.py</code>.</p>"},{"location":"configuration/#main-settings-file-chakshusettingspy","title":"Main Settings File: <code>chakshu/settings.py</code>","text":"<p>This is the central Django settings file. Key configurations to be aware of include:</p> <ul> <li> <p><code>SECRET_KEY</code>:</p> <ul> <li>A secret key for cryptographic signing.</li> <li>IMPORTANT: The one provided (<code>\"django-insecure-@ak_0lq74p123b5-vfo#4d(f+2blknu2(&amp;cz4c28or3g!cs^pq\"</code>) is insecure and generated by <code>startproject</code>. This MUST be replaced with a unique, secret key in production.</li> </ul> </li> <li> <p><code>DEBUG</code>:</p> <ul> <li><code>DEBUG = True</code> in the provided settings. This is suitable for development as it provides detailed error pages.</li> <li>IMPORTANT: Set <code>DEBUG = False</code> in production.</li> </ul> </li> <li> <p><code>ALLOWED_HOSTS</code>:</p> <ul> <li>Currently <code>[]</code>. For development, <code>['localhost', '127.0.0.1']</code> is common.</li> <li>IMPORTANT: In production, this MUST be set to the domain(s) that will host the application.</li> </ul> </li> <li> <p><code>INSTALLED_APPS</code>:</p> <ul> <li>Lists all Django applications active in this project:     <pre><code>INSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"corsheaders\",\n    \"api\",\n    \"captioner\",\n    \"core\",\n    \"scraper\",\n    \"summarizer\",\n]\n</code></pre></li> <li>Ensure any new apps are added here.</li> </ul> </li> <li> <p><code>MIDDLEWARE</code>:</p> <ul> <li>Includes <code>corsheaders.middleware.CorsMiddleware</code> for Cross-Origin Resource Sharing.</li> <li>Standard Django middleware is also present.</li> </ul> </li> <li> <p><code>CORS_ORIGIN_ALLOW_ALL = True</code>:</p> <ul> <li>This allows CORS requests from any origin. While convenient for development, for production, it's more secure to specify allowed origins using <code>CORS_ALLOWED_ORIGINS</code> or <code>CORS_ALLOWED_ORIGIN_REGEXES</code>.</li> <li>Example for production:     <pre><code># CORS_ORIGIN_ALLOW_ALL = False\n# CORS_ALLOWED_ORIGINS = [\n#     \"[https://yourfrontenddomain.com](https://yourfrontenddomain.com)\",\n#     \"http://localhost:3000\", # If you have a local frontend dev server\n# ]\n</code></pre></li> </ul> </li> <li> <p><code>ROOT_URLCONF = \"chakshu.urls\"</code>: Specifies the main URL configuration module.</p> </li> <li> <p><code>DATABASES</code>:</p> <ul> <li>Currently configured for SQLite:     <pre><code>DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n</code></pre></li> <li>This is suitable for development and small-scale deployments. For production, consider using a more robust database like PostgreSQL or MySQL, and update the configuration accordingly (ENGINE, NAME, USER, PASSWORD, HOST, PORT).</li> </ul> </li> <li> <p><code>CACHES</code>:</p> <ul> <li>Development: Uses local memory cache.     <pre><code>CACHES = {\n    \"default\": {\n        \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n        \"LOCATION\": \"image_captions_cache\", # Name for the cache instance\n        \"TIMEOUT\": 86400,  # Cache for 1 day (in seconds)\n    }\n}\n</code></pre></li> <li>Production (Commented Out): Redis cache configuration is provided but commented out.     <pre><code># CACHES = {\n#     \"default\": {\n#         \"BACKEND\": \"django_redis.cache.RedisCache\",\n#         \"LOCATION\": \"redis://127.0.0.1:6379/1\", # Your Redis server URI\n#         \"TIMEOUT\": 86400,  # 1 day\n#         \"OPTIONS\": {\n#             \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\",\n#         }\n#     }\n# }\n</code></pre>     To use Redis in production:<ol> <li>Install <code>django-redis</code>: <code>pip install django-redis</code>.</li> <li>Ensure a Redis server is running and accessible.</li> <li>Uncomment and update the <code>LOCATION</code> URI if your Redis server is elsewhere or uses a different database number or password.</li> </ol> </li> </ul> </li> <li> <p><code>STATIC_URL = \"static/\"</code>: URL prefix for static files.</p> </li> <li> <p><code>DEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"</code>: Specifies the default primary key type.</p> </li> </ul>"},{"location":"configuration/#custom-configuration-file-configpy","title":"Custom Configuration File: <code>config.py</code>","text":"<p>A <code>config.py</code> file exists at the project root. The contents of this file were not provided. Developers should inspect <code>config.py</code> to understand what configurations it holds. It might be used for:</p> <ul> <li>API keys for external services (e.g., Google Search API if the <code>googlesearch</code> library requires one for higher volume, or other third-party APIs).</li> <li>Paths to machine learning models if used by <code>captioner</code> or <code>summarizer</code>.</li> <li>Service URLs not suitable for <code>settings.py</code>.</li> <li>Custom application-specific settings.</li> </ul> <p>It's common to import values from such a <code>config.py</code> into <code>settings.py</code> or other relevant modules. Ensure that sensitive information in <code>config.py</code> (like API keys) is not committed to version control (e.g., by adding <code>config.py</code> to <code>.gitignore</code> and using a <code>config.py.example</code> template).</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>For production, it is best practice to manage sensitive settings (like <code>SECRET_KEY</code>, database credentials, API keys) using environment variables rather than hardcoding them in <code>settings.py</code> or <code>config.py</code>. Libraries like <code>python-decouple</code> or <code>django-environ</code> can help with this.</p> <p>Example using environment variables (conceptual): ```python</p>"},{"location":"configuration/#settingspy","title":"settings.py","text":"<p>import os</p>"},{"location":"configuration/#from-decouple-import-config-if-using-python-decouple","title":"from decouple import config # if using python-decouple","text":"<p>SECRET_KEY = os.environ.get(\"DJANGO_SECRET_KEY\", \"default-insecure-key-for-dev\") DATABASE_PASSWORD = os.environ.get(\"DB_PASSWORD\")</p>"},{"location":"configuration/#databasesdefaultpassword-database_password","title":"DATABASES['default']['PASSWORD'] = DATABASE_PASSWORD","text":""},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-is-wikipedia-content-fetched","title":"How is Wikipedia content fetched?","text":"<p>Through both Google Search (limited to <code>site:en.wikipedia.org</code>) and the MediaWiki API, with scraping as fallback.</p>"},{"location":"faq/#can-i-replace-the-search-backend","title":"Can I replace the search backend?","text":"<p>Yes. Replace the <code>googlesearch</code> logic with Bing or an internal search index.</p>"},{"location":"faq/#is-the-image-captioning-module-required","title":"Is the image captioning module required?","text":"<p>Not for basic summary usage. It is only triggered with option <code>4</code> in <code>/process/</code>.</p>"},{"location":"project_overview/","title":"Project Overview","text":""},{"location":"project_overview/#introduction","title":"Introduction","text":"<p>Chakshu is a Django-based backend application designed to interact with web content, specifically focusing on Wikipedia articles. It allows users to search for articles, select a specific article, and then choose from various options to process and retrieve different types of information from that article. This includes summaries, full content, image captions, and citations.</p>"},{"location":"project_overview/#core-functionality","title":"Core Functionality","text":"<p>The primary workflow of the Chakshu application involves:</p> <ol> <li>Searching: Users can input a search query. Chakshu uses Google Search (scoped to <code>en.wikipedia.org</code>) to find relevant Wikipedia articles.</li> <li>Selection: From the search results, the user (or a frontend client) selects a specific Wikipedia article URL.</li> <li>Processing Options: For the selected article, Chakshu offers several processing options:<ul> <li>Read a short description.</li> <li>Read a summary of the page.</li> <li>Read the full content of the page.</li> <li>Read captions of images present on the page.</li> <li>Read all references (citations) present on the page.</li> </ul> </li> <li>Content Retrieval &amp; Delivery: Based on the chosen option, Chakshu fetches, processes, and returns the requested information in a structured JSON format.</li> </ol>"},{"location":"project_overview/#key-technologies","title":"Key Technologies","text":"<ul> <li>Django: The core web framework used for building the application.</li> <li>Django REST framework (Implicit): While not explicitly stated, the API structure suggests its use or a similar pattern for building web APIs.</li> <li>Googlesearch-python: Python library used for performing Google searches.</li> <li>Joblib: For parallel processing, enhancing performance in tasks like fetching multiple short descriptions.</li> <li>Requests (likely): For making HTTP requests to fetch web content (assumed to be used by scraper and wiki_api).</li> <li>Beautiful Soup (likely): For parsing HTML content (assumed to be used by the scraper module).</li> <li>MediaWiki API (via <code>core/wiki_api.py</code>): For fetching structured data like page summaries from Wikipedia.</li> <li>SQLite: Default database for development.</li> <li>Redis (optional): Configured for production caching.</li> <li>Docker: A <code>Dockerfile</code> is present, indicating containerization capability.</li> </ul>"},{"location":"project_overview/#target-audience","title":"Target Audience","text":"<p>This application serves as a backend for clients (e.g., web or mobile applications) that need to provide users with processed information from Wikipedia articles in a simplified and structured manner.</p>"},{"location":"project_overview/#future-scope-potential","title":"Future Scope (Potential)","text":"<ul> <li>Expanding to other knowledge sources beyond Wikipedia.</li> <li>Advanced NLP processing on the retrieved content (e.g., question answering, entity extraction).</li> <li>More sophisticated caching and data persistence strategies.</li> <li>User authentication and personalized features.</li> </ul>"},{"location":"project_structure/","title":"Project Structure","text":"<p>The Chakshu project follows a standard Django project layout. Here's an overview of the key directories and files:</p> <pre><code>chakshu/\n\u251c\u2500\u2500 api/                      # Django app: Handles API routing and structure\n\u2502   \u251c\u2500\u2500 admin.py\n\u2502   \u251c\u2500\u2500 apps.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 models.py             # Database models for the 'api' app (if any)\n\u2502   \u251c\u2500\u2500 tests.py\n\u2502   \u251c\u2500\u2500 urls.py               # URL routing for the 'api' app\n\u2502   \u2514\u2500\u2500 views.py              # Views for the 'api' app (if any specific logic)\n\u251c\u2500\u2500 captioner/                # Django app: Responsible for image captioning\n\u2502   \u251c\u2500\u2500 admin.py\n\u2502   \u251c\u2500\u2500 apps.py\n\u2502   \u251c\u2500\u2500 image_captioner.py    # Core logic for image captioning\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 metadata_image_captioner.py # Alternative/additional captioning logic\n\u2502   \u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 models.py             # Models for storing caption data (e.g., cached captions)\n\u2502   \u251c\u2500\u2500 tests.py\n\u2502   \u2514\u2500\u2500 views.py              # Views/functions like fetch_and_process_images\n\u251c\u2500\u2500 chakshu/                  # Main Django project directory\n\u2502   \u251c\u2500\u2500 asgi.py               # ASGI config for asynchronous serving\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 settings.py           # Django project settings\n\u2502   \u251c\u2500\u2500 urls.py               # Root URL configuration for the project\n\u2502   \u2514\u2500\u2500 wsgi.py               # WSGI config for synchronous serving\n\u251c\u2500\u2500 config.py                 # Custom configuration file (contents unknown)\n\u251c\u2500\u2500 core/                     # Django app: Core business logic and orchestration\n\u2502   \u251c\u2500\u2500 admin.py\n\u2502   \u251c\u2500\u2500 apps.py\n\u2502   \u251c\u2500\u2500 hyperlinks.py         # (Contents unknown, likely utility for links)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 models.py             # Models for core entities (e.g., search history, pages)\n\u2502   \u251c\u2500\u2500 tests.py\n\u2502   \u251c\u2500\u2500 urls.py               # URL routing for the 'core' app API endpoints\n\u2502   \u251c\u2500\u2500 views.py              # Main API views (SearchResultsView, SelectLinkView, ProcessOptionView)\n\u2502   \u2514\u2500\u2500 wiki_api.py           # Wrapper for MediaWiki API interactions\n\u251c\u2500\u2500 demo.py                   # Standalone demonstration script (contents unknown)\n\u251c\u2500\u2500 Dockerfile                # Instructions for building a Docker image\n\u251c\u2500\u2500 __init__.py               # Makes the root 'chakshu' a Python package\n\u251c\u2500\u2500 manage.py                 # Django's command-line utility\n\u251c\u2500\u2500 output.txt                # Generic output file (likely for temporary results or logs)\n\u251c\u2500\u2500 README.md                 # Project README file, usually contains overview and setup\n\u2514\u2500\u2500 scraper/                  # Django app: Handles web scraping, particularly from Wikipedia\n    \u251c\u2500\u2500 admin.py\n    \u251c\u2500\u2500 apps.py\n    \u251c\u2500\u2500 imageScrapper.py      # Logic for scraping images (contents unknown)\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 migrations/\n    \u251c\u2500\u2500 models.py             # Models for scraped data (if any)\n    \u251c\u2500\u2500 scraper.py            # Core WikipediaScraper class logic\n    \u251c\u2500\u2500 tests.py\n    \u251c\u2500\u2500 urls.py               # URL routing for 'scraper' app (if any direct endpoints)\n    \u2514\u2500\u2500 views.py              # Helper views/functions for scraping tasks\n\u2514\u2500\u2500 summarizer/               # Django app: (Functionality to be inferred, likely text summarization)\n    # (Expected files: models.py, views.py, etc.)\n</code></pre>"},{"location":"project_structure/#key-components","title":"Key Components","text":""},{"location":"project_structure/#django-project-chakshu","title":"Django Project (<code>chakshu/</code>)","text":"<ul> <li><code>settings.py</code>: Contains all project configurations, including <code>INSTALLED_APPS</code>, <code>MIDDLEWARE</code>, <code>DATABASES</code>, <code>CACHES</code>, <code>SECRET_KEY</code>, etc. This is a crucial file for understanding how the project is wired up.</li> <li><code>urls.py</code>: The main URL router. It includes <code>admin.site.urls</code> and delegates API-related URLs to the <code>api</code> app (<code>api.urls</code>).</li> </ul>"},{"location":"project_structure/#django-apps","title":"Django Apps","text":"<ul> <li><code>api</code>:</li> <li>Acts as a primary entry point or namespace for the API.</li> <li> <p><code>api/urls.py</code> includes URLs from <code>core.urls</code> and <code>scraper.urls</code>. This suggests it might serve as an aggregator or API versioning layer.</p> </li> <li> <p><code>core</code>:</p> </li> <li>Contains central API logic.</li> <li><code>core/views.py</code> has the main views that handle client requests for search, selection, and processing.</li> <li><code>core/urls.py</code> defines endpoints like <code>/search/</code>, <code>/select/</code>, and <code>/process/</code>.</li> <li> <p><code>core/wiki_api.py</code> likely acts as a dedicated MediaWiki API client to fetch structured data.</p> </li> <li> <p><code>scraper</code>:</p> </li> <li>Manages web scraping tasks, focused on Wikipedia.</li> <li><code>scraper/scraper.py</code> likely contains the <code>WikipediaScraper</code> implementation.</li> <li> <p><code>scraper/views.py</code> defines helper functions like <code>get_short_description</code>, <code>get_citations</code>, and <code>get_full_content</code>, used by <code>core.views</code>.</p> </li> <li> <p><code>captioner</code>:</p> </li> <li>Generates or retrieves image captions.</li> <li><code>captioner/image_captioner.py</code> and <code>captioner/metadata_image_captioner.py</code> contain the captioning logic.</li> <li> <p><code>captioner/views.py</code> includes endpoints like <code>Workspace_and_process_images</code>, likely called by <code>core.views</code>.</p> </li> <li> <p><code>summarizer</code>:</p> </li> <li>Present in <code>INSTALLED_APPS</code>, but specific functionality is not shown.</li> <li>Presumably handles summarization of scraped text content.</li> </ul>"},{"location":"project_structure/#other-important-files","title":"Other Important Files","text":"<ul> <li><code>manage.py</code>: Utility script for running commands such as <code>runserver</code>, <code>makemigrations</code>, and <code>migrate</code>.</li> <li><code>Dockerfile</code>: Contains instructions to build a Docker image for the application.</li> <li><code>config.py</code>: A custom config file \u2014 possibly contains API keys, paths, or constants.</li> <li><code>README.md</code>: Project overview and setup instructions.</li> <li><code>demo.py</code>: Possibly demonstrates a specific feature or flow without requiring the full server to run.</li> </ul> <p>Understanding this structure will help developers navigate the codebase, debug issues efficiently, and contribute new features effectively.</p>"},{"location":"setup_guide/","title":"Chakshu","text":"<p>Chakshu helps blind people read and navigate Wikipedia articles using voice commands.</p>"},{"location":"setup_guide/#getting-started","title":"Getting Started","text":""},{"location":"setup_guide/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the following installed on your machine:</p> <ul> <li>Python 3.10+</li> <li>Poetry</li> </ul>"},{"location":"setup_guide/#installation","title":"Installation","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/DhruvGoyal375/chakshu.git\ncd chakshu\n</code></pre> <ol> <li>Set up Poetry to create virtual environments within the project directory:</li> </ol> <pre><code>poetry config virtualenvs.in-project true\n</code></pre> <ol> <li>Install dependencies using Poetry:</li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>poetry shell\n</code></pre>"},{"location":"setup_guide/#running-the-project","title":"Running the Project","text":"<p>Once the virtual environment is activated, you can run the project with:</p> <pre><code>python chakshu/manage.py runserver\n</code></pre>"},{"location":"setup_guide/#contact","title":"Contact","text":"<p>If you have any questions or need further assistance, feel free to open an issue or contact the maintainers.</p>"},{"location":"structure/","title":"Project Structure","text":"<pre><code>chakshu/\n\u251c\u2500\u2500 api/              # Main API router\n\u251c\u2500\u2500 captioner/        # Image captioning module\n\u251c\u2500\u2500 core/             # Wikipedia search &amp; content processing logic\n\u251c\u2500\u2500 scraper/          # HTML scraping and data extraction logic\n\u251c\u2500\u2500 chakshu/          # Django project config\n\u251c\u2500\u2500 manage.py         # Django CLI runner\n\u251c\u2500\u2500 Dockerfile        # Optional: Docker container definition\n</code></pre>"},{"location":"structure/#key-files","title":"Key Files","text":"<ul> <li><code>chakshu/settings.py</code>: Django settings file  </li> <li><code>chakshu/urls.py</code>: Root URL router  </li> <li><code>api/urls.py</code>: Includes <code>core</code> and <code>scraper</code> APIs  </li> </ul>"},{"location":"api/","title":"API Reference Introduction","text":"<p>The Chakshu application provides a RESTful API for searching Wikipedia articles and processing their content. This section details the available endpoints, request parameters, and response formats.</p> <p>All API endpoints are prefixed by <code>/api/</code>. Based on the provided URL configurations: - Main project URLs (<code>chakshu/urls.py</code>) route <code>/api/</code> to <code>api.urls</code>. - The <code>api/urls.py</code> then includes URLs from <code>core.urls</code> and <code>scraper.urls</code>.</p> <p>The primary user-facing API endpoints are defined within the <code>core</code> app.</p>"},{"location":"api/#base-url","title":"Base URL","text":"<p>The API endpoints described here assume a base URL, which during local development is typically: <code>http://127.0.0.1:8000/api/</code></p>"},{"location":"api/#authentication","title":"Authentication","text":"<p>The provided code snippets do not explicitly show any authentication mechanisms for these public-facing APIs. They appear to be open endpoints.</p>"},{"location":"api/#common-responses","title":"Common Responses","text":"<ul> <li>Success: Typically <code>200 OK</code> with a JSON body.</li> <li>Client Error: <code>400 Bad Request</code> for missing or invalid parameters, with a JSON body containing an <code>error</code> message.</li> <li>Server Error: <code>500 Internal Server Error</code> if something goes wrong on the server side.</li> </ul>"},{"location":"api/#api-endpoints","title":"API Endpoints","text":"<p>Detailed descriptions of each endpoint can be found in the following pages:</p> <ul> <li>Search Endpoint: For searching Wikipedia articles.</li> <li>Content Processing Endpoints: For selecting an article and processing its content.</li> </ul> <p>Note: While <code>scraper.urls</code> is included in <code>api.urls</code>, the primary documented flow uses endpoints from <code>core.urls</code>. Any direct endpoints exposed by the scraper app would need separate documentation if intended for external use.</p>"},{"location":"api/endpoints/","title":"API Endpoints","text":""},{"location":"api/endpoints/#apisearch","title":"<code>/api/search/</code>","text":"<ul> <li>Method: <code>GET</code> </li> <li>Query param: <code>q=your+query</code> </li> <li>Function:   Performs a Google search restricted to Wikipedia.   Returns top 5 links with short descriptions parsed in parallel.</li> </ul>"},{"location":"api/endpoints/#apiselect","title":"<code>/api/select/</code>","text":"<ul> <li>Method: <code>GET</code> </li> <li>Query param: <code>link=selected_wiki_url</code> </li> <li> <p>Function:   Accepts a selected Wikipedia link and returns available processing options:</p> </li> <li> <p>Short description  </p> </li> <li>Summary  </li> <li>Full content  </li> <li>Image captions  </li> <li>References</li> </ul>"},{"location":"api/endpoints/#apiprocess","title":"<code>/api/process/</code>","text":"<ul> <li>Method: <code>GET</code> </li> <li>Query params: </li> <li><code>link=wiki_url</code> </li> <li> <p><code>option=1</code> to <code>5</code></p> </li> <li> <p>Function:   Depending on the selected option:</p> </li> <li> <p>Short description (from page HTML)  </p> </li> <li>Summary (via MediaWiki API)  </li> <li>Full content (via HTML scraping)  </li> <li>All image captions on the page  </li> <li>Citations/references from the page</li> </ul>"},{"location":"api/processing_api/","title":"Content Processing Endpoints","text":"<p>Once a Wikipedia article URL is obtained (e.g., from the Search Endpoint), these endpoints allow clients to choose how to process and retrieve information from that article.</p>"},{"location":"api/processing_api/#1-get-apiselect","title":"1. <code>GET /api/select/</code>","text":"<p>Provides a list of available processing options for a given Wikipedia article link.</p>"},{"location":"api/processing_api/#request","title":"Request","text":"<ul> <li>Method: <code>GET</code></li> <li>URL: <code>/api/select/</code></li> <li>Query Parameters:<ul> <li><code>link</code> (string, required): The full URL of the Wikipedia article to process.</li> </ul> </li> </ul>"},{"location":"api/processing_api/#example-request","title":"Example Request","text":"<p>GET http://127.0.0.1:8000/api/select/?link=https://en.wikipedia.org/wiki/Artificial_intelligence</p>"},{"location":"api/processing_api/#response-success-200-ok","title":"Response (Success - <code>200 OK</code>)","text":"<p>Returns a JSON object containing a message and a list of processing options.</p> <ul> <li>Body:     <pre><code>{\n    \"message\": \"Select an option\",\n    \"options\": [\n        \"Read short description of the page.\",\n        \"Read summary of the page.\",\n        \"Read the whole page.\",\n        \"Read only captions of the images present on the page.\",\n        \"Read all the References present on the page.\"\n    ]\n}\n</code></pre><ul> <li><code>message</code>: A guiding message for the user/client.</li> <li><code>options</code>: A list of strings describing the available actions. The client would typically map these to numerical options (1-5) for the <code>/api/process/</code> endpoint.</li> </ul> </li> </ul>"},{"location":"api/processing_api/#response-error-400-bad-request","title":"Response (Error - <code>400 Bad Request</code>)","text":"<p>If the <code>link</code> parameter is missing.</p> <ul> <li>Body:     <pre><code>{\n    \"error\": \"Please provide the 'link' parameter.\"\n}\n</code></pre></li> </ul>"},{"location":"api/processing_api/#implementation-details","title":"Implementation Details","text":"<ul> <li>Located in <code>core.views.SelectLinkView</code>.</li> <li>Returns a static list of options.</li> </ul>"},{"location":"api/processing_api/#2-get-apiprocess","title":"2. <code>GET /api/process/</code>","text":"<p>Processes the selected Wikipedia article based on a chosen option number.</p>"},{"location":"api/processing_api/#request_1","title":"Request","text":"<ul> <li>Method: <code>GET</code></li> <li>URL: <code>/api/process/</code></li> <li>Query Parameters:<ul> <li><code>link</code> (string, required): The full URL of the Wikipedia article.</li> <li><code>option</code> (integer, required): A number (1-5) corresponding to the desired processing action:<ul> <li><code>1</code>: Get short description.</li> <li><code>2</code>: Get page summary.</li> <li><code>3</code>: Get full page content.</li> <li><code>4</code>: Get captions of all images.</li> <li><code>5</code>: Get references/citations.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/processing_api/#example-request_1","title":"Example Request","text":"<p>To get the summary (option 2) for an article:</p> <p>GET http://127.0.0.1:8000/api/process/?link=https://en.wikipedia.org/wiki/Artificial_intelligence&amp;option=2</p>"},{"location":"api/processing_api/#response-success-200-ok_1","title":"Response (Success - <code>200 OK</code>)","text":"<p>The response format varies based on the <code>option</code> selected.</p> <ul> <li>For <code>option=1</code> (Short Description):     <pre><code>{\n    \"short_description\": \"Intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans.\"\n}\n</code></pre></li> <li>For <code>option=2</code> (Summary):     <pre><code>{\n    \"summary\": \"Artificial intelligence (AI) is intelligence\u2014perceiving, synthesizing, and inferring information\u2014demonstrated by machines, as opposed to intelligence displayed by animals and humans. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\"\n}\n</code></pre></li> <li>For <code>option=3</code> (Full Content):     <pre><code>{\n    \"text\": \"The full textual content of the Wikipedia page...\"\n}\n</code></pre></li> <li>For <code>option=4</code> (Image Captions):     <pre><code>{\n    \"text\": [\n        \"Caption for image 1...\",\n        \"Caption for image 2...\"\n    ]\n}\n</code></pre></li> <li>For <code>option=5</code> (Citations):     <pre><code>{\n    \"text\": [\n        \"Citation 1 details...\",\n        \"Citation 2 details...\"\n    ]\n}\n</code></pre></li> </ul>"},{"location":"api/processing_api/#response-error-400-bad-request_1","title":"Response (Error - <code>400 Bad Request</code>)","text":"<ul> <li>If <code>link</code> or <code>option</code> parameters are missing:     <pre><code>{\n    \"error\": \"Please provide both 'link' and 'option' parameters.\"\n}\n</code></pre></li> <li>If <code>option</code> is not a valid number:     <pre><code>{\n    \"error\": \"Invalid option. Please provide a valid number for the option.\"\n}\n</code></pre></li> <li>If <code>option</code> is a number but not in the valid range (1-5):     <pre><code>{\n    \"error\": \"Invalid option. Please select a valid option (1-5).\"\n}\n</code></pre></li> </ul>"},{"location":"api/processing_api/#implementation-details_1","title":"Implementation Details","text":"<ul> <li>Located in <code>core.views.ProcessOptionView</code>.</li> <li>Extracts <code>page_title</code> from the <code>link</code>.</li> <li>Calls different services based on the <code>option</code>:<ul> <li>Option 1: <code>scraper.views.get_short_description(selected_link)</code></li> <li>Option 2: <code>core.wiki_api.mediawiki_api.get_page_summary(page_title)</code></li> <li>Option 3: <code>scraper.views.get_full_content(selected_link)</code></li> <li>Option 4: <code>captioner.views.fetch_and_process_images(selected_link)</code></li> <li>Option 5: <code>scraper.views.get_citations(selected_link)</code></li> </ul> </li> </ul>"},{"location":"api/scraper/","title":"Scraper Utilities","text":"<p>The <code>scraper</code> module uses a class <code>WikipediaScraper</code> that handles: - Fetching page HTML using requests - Cleaning tags - Extracting summary, citations, and image URLs</p>"},{"location":"api/scraper/#key-functions","title":"Key Functions","text":"<ul> <li><code>get_short_description(url)</code></li> <li><code>get_full_content(url)</code></li> <li><code>get_citations(url)</code></li> <li><code>get_image_urls(url)</code></li> </ul>"},{"location":"api/search_api/","title":"Search Endpoint","text":"<p>This endpoint allows clients to search for Wikipedia articles based on a query.</p>"},{"location":"api/search_api/#get-apisearch","title":"<code>GET /api/search/</code>","text":"<p>Searches for Wikipedia articles matching the provided query string.</p>"},{"location":"api/search_api/#request","title":"Request","text":"<ul> <li>Method: <code>GET</code></li> <li>URL: <code>/api/search/</code></li> <li>Query Parameters:<ul> <li><code>q</code> (string, required): The search query. Spaces should be URL-encoded (e.g., <code>%20</code>).</li> </ul> </li> </ul>"},{"location":"api/search_api/#example-request","title":"Example Request","text":"<p>GET http://127.0.0.1:8000/api/search/?q=artificial%20intelligence</p>"},{"location":"api/search_api/#response-success-200-ok","title":"Response (Success - <code>200 OK</code>)","text":"<p>Returns a JSON object containing a message and a list of search results.</p> <ul> <li>Body:     <pre><code>{\n    \"message\": \"Select the article you want to read\",\n    \"results\": [\n        {\n            \"index\": 1,\n            \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n            \"title\": \"Artificial intelligence\",\n            \"short_description\": \"Intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans.\"\n        },\n        {\n            \"index\": 2,\n            \"url\": \"https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence\",\n            \"title\": \"Applications of artificial intelligence\",\n            \"short_description\": \"Overview of the various applications of artificial intelligence (AI)\"\n        }\n        // ... more results up to 5\n    ]\n}\n</code></pre><ul> <li><code>message</code>: A guiding message for the user/client.</li> <li><code>results</code>: A list of article objects.<ul> <li><code>index</code>: A 1-based index for the result.</li> <li><code>url</code>: The URL of the Wikipedia article.</li> <li><code>title</code>: The title of the Wikipedia article, derived from the URL.</li> <li><code>short_description</code>: A brief description of the article, fetched from the Wikipedia page.</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/search_api/#response-error-400-bad-request","title":"Response (Error - <code>400 Bad Request</code>)","text":"<p>If the <code>q</code> parameter is missing or empty.</p> <ul> <li>Body:     <pre><code>{\n    \"error\": \"Please provide a search query using the 'q' parameter.\"\n}\n</code></pre></li> </ul>"},{"location":"api/search_api/#implementation-details","title":"Implementation Details","text":"<ul> <li>Located in <code>core.views.SearchResultsView</code>.</li> <li>Uses the <code>googlesearch</code> library to perform a site-specific search on <code>en.wikipedia.org</code> (limited to 5 results).</li> <li>Removes duplicate URLs.</li> <li>Fetches short descriptions for each unique URL in parallel using <code>joblib</code> and the <code>scraper.views.get_short_description</code> function.</li> </ul>"},{"location":"api/views/","title":"Core Views","text":""},{"location":"api/views/#searchresultsview","title":"SearchResultsView","text":"<p>Uses Google Search to find Wikipedia articles. Calls <code>get_short_description()</code> on results.</p>"},{"location":"api/views/#selectlinkview","title":"SelectLinkView","text":"<p>Provides options to the user after they choose a link.</p>"},{"location":"api/views/#processoptionview","title":"ProcessOptionView","text":"<p>Fetches content based on selected option: - 1: Short Description - 2: Summary using MediaWiki API - 3: Full scraped content - 4: Captions from image processing - 5: Reference scraping</p>"},{"location":"modules/","title":"Modules Overview","text":"<p>The Chakshu project is organized into several Django apps, each responsible for a distinct set of functionalities. This modular design promotes separation of concerns and makes the codebase easier to manage and scale.</p>"},{"location":"modules/#django-apps","title":"Django Apps","text":"<p>The primary applications within this project are:</p> <ul> <li> <p>Core App (<code>core</code>):     Handles the main API logic, orchestrates calls to other services/apps, and manages interactions with external APIs like the MediaWiki API. It defines the primary user-facing API endpoints.</p> </li> <li> <p>Scraper App (<code>scraper</code>):     Responsible for all web scraping activities. It contains the logic to fetch and parse content from Wikipedia pages, such as short descriptions, full text, and citations.</p> </li> <li> <p>Captioner App (<code>captioner</code>):     Dedicated to generating or retrieving captions for images found on web pages.</p> </li> <li> <p>API App (<code>api</code>):     Serves as a routing and namespacing layer for the project's API. It aggregates URL patterns from other apps like <code>core</code> and <code>scraper</code>.</p> </li> </ul>"},{"location":"modules/#inter-app-communication","title":"Inter-App Communication","text":"<p>The <code>core</code> app often acts as the central coordinator. For example, when a user requests to process a Wikipedia page, the <code>core.views.ProcessOptionView</code> will:</p> <ul> <li>Call functions from <code>scraper.views</code> to get scraped content.</li> <li>Call methods from <code>core.wiki_api.WikiAPI</code> (which is part of the <code>core</code> app but interacts with an external service) for summaries.</li> <li>Call functions from <code>captioner.views</code> to get image captions.</li> </ul> <p>This layered approach allows each app to specialize in its domain while contributing to the overall functionality of the application.</p>"},{"location":"modules/#exploring-further","title":"Exploring Further","text":"<p>For detailed information on each app, please refer to their respective pages:</p> <ul> <li>Core App (<code>core</code>)</li> <li>Scraper App (<code>scraper</code>)</li> <li>Captioner App (<code>captioner</code>)</li> <li>API App (<code>api</code>)</li> </ul>"},{"location":"modules/api_app/","title":"API App (<code>api</code>)","text":"<p>The <code>api</code> app in the Chakshu project primarily serves as a routing and namespacing layer for the various API endpoints provided by the application.</p>"},{"location":"modules/api_app/#responsibilities","title":"Responsibilities","text":"<ul> <li>API URL Aggregation: It groups API-related URL patterns from other apps under a common prefix (e.g., <code>/api/</code>).</li> <li>Versioning (Potential): While not explicitly shown, an app like this is often used to implement API versioning (e.g., <code>/api/v1/</code>, <code>/api/v2/</code>).</li> <li>Global API Configuration (Potential): Could house middleware or settings specific to the API, although most of this seems to be handled at the project level in <code>chakshu/settings.py</code>.</li> </ul>"},{"location":"modules/api_app/#key-files-and-components","title":"Key Files and Components","text":"<ul> <li> <p><code>urls.py</code>:</p> <ul> <li>This is the most important file in the <code>api</code> app based on the provided information.</li> <li>Its <code>urlpatterns</code> include URL configurations from other apps:     <pre><code>from django.urls import include, path\n\nurlpatterns = [\n    path(\"\", include(\"core.urls\")),\n    path(\"\", include(\"scraper.urls\")),\n]\n</code></pre></li> <li>This means that any URLs defined in <code>core.urls.py</code> and <code>scraper.urls.py</code> will be accessible under the path where <code>api.urls</code> is itself included (which is <code>/api/</code> as defined in <code>chakshu/urls.py</code>).</li> <li>For example, if <code>core.urls</code> has <code>path(\"search/\", ...)</code>, the full path becomes <code>/api/search/</code>.</li> </ul> </li> <li> <p><code>views.py</code>:</p> <ul> <li>The provided file structure lists <code>api/views.py</code>.</li> <li>It's possible this file is empty or contains minimal views if the app's primary role is routing.</li> <li>Alternatively, it could contain high-level API views, such as an API root view that lists available endpoints, or views related to API-wide concerns like authentication if that were managed here.</li> </ul> </li> <li> <p><code>models.py</code>, <code>admin.py</code>, <code>apps.py</code>, <code>tests.py</code>:</p> <ul> <li>These are standard Django app files.</li> <li><code>models.py</code>: Unlikely to contain significant models if the app is purely for routing, but could be used for API logging or metadata.</li> <li>The other files would serve their standard purposes.</li> </ul> </li> </ul>"},{"location":"modules/api_app/#how-it-fits-in","title":"How It Fits In","text":"<ol> <li>A request comes to the Django project (e.g., <code>http://127.0.0.1:8000/api/search/?q=test</code>).</li> <li>The main <code>chakshu/urls.py</code> matches the <code>api/</code> prefix and passes the rest of the path (<code>search/?q=test</code>) to <code>api.urls</code>.     <pre><code># chakshu/urls.py\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n    path(\"api/\", include(\"api.urls\")), # &lt;--- Match\n]\n</code></pre></li> <li><code>api/urls.py</code> then tries to match the remaining path. In this example, <code>path(\"\", include(\"core.urls\"))</code> would delegate the <code>search/</code> part to <code>core.urls.py</code>.</li> <li><code>core.urls.py</code> finally matches <code>search/</code> to <code>SearchResultsView</code>.</li> </ol>"},{"location":"modules/api_app/#development-considerations","title":"Development Considerations","text":"<ul> <li>Clarity of Routing: Ensure that the includes in <code>api/urls.py</code> are clear and don't lead to ambiguous or conflicting URL patterns.</li> <li>API Versioning: If the API is expected to evolve significantly, consider implementing a versioning strategy using this app (e.g., by creating sub-namespaces like <code>path(\"v1/\", include(\"core.v1.urls\"))</code>).</li> <li>Minimal Logic: Generally, an app dedicated to API routing should contain minimal business logic itself, delegating that to the apps providing the actual features.</li> </ul>"},{"location":"modules/captioner/","title":"Captioner Module","text":"<p>This module processes images scraped from Wikipedia and generates captions for them.</p>"},{"location":"modules/captioner/#main-class","title":"Main Class","text":"<ul> <li><code>image_captioner.py</code>: Handles image downloading and preprocessing</li> <li><code>metadata_image_captioner.py</code>: Generates context-aware image captions</li> </ul> <p>Currently uses local models. Can be extended for cloud-based captioning (e.g., Vertex AI).</p>"},{"location":"modules/captioner_app/","title":"Captioner App (<code>captioner</code>)","text":"<p>The <code>captioner</code> app is responsible for providing captions for images found on Wikipedia pages.</p>"},{"location":"modules/captioner_app/#responsibilities","title":"Responsibilities","text":"<ul> <li>Image Identification: Potentially identifying images on a page (though image URLs might be passed from the <code>scraper</code> app).</li> <li>Caption Retrieval/Generation:<ul> <li>Fetching existing captions (e.g., from HTML <code>alt</code> text, <code>figcaption</code> elements, or surrounding text).</li> <li>Utilizing metadata associated with images, if available (as suggested by <code>metadata_image_captioner.py</code>).</li> <li>Possibly integrating with AI models for generating captions if no explicit captions are found (though this is an advanced feature not explicitly shown).</li> </ul> </li> <li>Data Formatting: Presenting the captions in a usable format.</li> </ul>"},{"location":"modules/captioner_app/#key-files-and-components","title":"Key Files and Components","text":"<ul> <li> <p><code>views.py</code>:</p> <ul> <li><code>Workspace_and_process_images(selected_link)</code>:<ul> <li>This is the primary function exposed by the <code>captioner</code> app, called by <code>core.views.ProcessOptionView</code> when <code>option=4</code> is selected.</li> <li>The exact implementation is not provided, but it would involve:<ol> <li>Getting image URLs for the <code>selected_link</code> (likely by calling a function from the <code>scraper</code> app, e.g., <code>scraper.views.get_image_urls()</code>).</li> <li>For each image, attempting to find or generate a caption using logic from <code>image_captioner.py</code> and/or <code>metadata_image_captioner.py</code>.</li> <li>Aggregating these captions and returning them. The return format is <code>ls_captions</code>, which is then passed to <code>JsonResponse</code> in <code>core.views</code>.</li> </ol> </li> </ul> </li> </ul> </li> <li> <p><code>image_captioner.py</code>:</p> <ul> <li>This file is expected to contain the core logic for deriving captions from image context or simple attributes.</li> <li>This might involve:<ul> <li>Scraping the HTML around an image tag.</li> <li>Looking for <code>&lt;figcaption&gt;</code> elements.</li> <li>Using <code>alt</code> text or <code>title</code> attributes of <code>&lt;img&gt;</code> tags.</li> <li>Basic heuristics based on surrounding text.</li> </ul> </li> <li>Developer Note: The specific strategies used in this file will determine the quality and source of the captions.</li> </ul> </li> <li> <p><code>metadata_image_captioner.py</code>:</p> <ul> <li>This file suggests an alternative or supplementary approach to captioning, focusing on metadata.</li> <li>This could involve:<ul> <li>Fetching image metadata if images are hosted on platforms like Wikimedia Commons, which often have structured descriptions.</li> <li>Parsing EXIF data if accessible (less likely in a web context).</li> </ul> </li> <li>Developer Note: Understanding how this module complements or differs from <code>image_captioner.py</code> is important.</li> </ul> </li> <li> <p><code>models.py</code>:</p> <ul> <li>The file structure shows <code>captioner/models.py</code> and a migration <code>0001_initial.py</code>. This implies that the <code>captioner</code> app might store data, such as:<ul> <li>Cached captions to avoid re-processing.</li> <li>Associations between image URLs and their captions.</li> </ul> </li> <li>Developers should inspect the current models to understand what data is persisted.</li> </ul> </li> <li> <p><code>admin.py</code>, <code>apps.py</code>, <code>tests.py</code>: Standard Django app files. <code>admin.py</code> would be used to register any <code>captioner</code> models with the Django admin interface.</p> </li> </ul>"},{"location":"modules/captioner_app/#interaction-with-other-modules","title":"Interaction with Other Modules","text":"<ul> <li><code>core</code> app: The <code>core.views.ProcessOptionView</code> calls <code>captioner.views.fetch_and_process_images</code>.</li> <li><code>scraper</code> app (Likely): The <code>captioner</code> app will likely need image URLs from the <code>scraper</code> app to know which images to process. This could be a direct function call (e.g., <code>captioner.views</code> calling <code>scraper.views.get_image_urls()</code>) or image URLs could be passed into <code>Workspace_and_process_images</code>.</li> </ul>"},{"location":"modules/captioner_app/#development-considerations","title":"Development Considerations","text":"<ul> <li>Caption Sources: Determine the priority and reliability of different caption sources (alt text, figcaption, metadata, generated).</li> <li>Accuracy: The accuracy of captions is crucial. If generating captions, the quality of the model or heuristics will be important.</li> <li>Performance: Captioning multiple images can be time-consuming. Consider:<ul> <li>Batch processing.</li> <li>Caching results in <code>captioner.models</code>.</li> <li>Asynchronous processing if caption generation is slow (though the current <code>core.views</code> call is synchronous).</li> </ul> </li> <li>Handling Missing Captions: Define a clear strategy for images where no caption can be found.</li> <li>External Libraries/Services: If using advanced AI captioning models, manage those dependencies and any associated API keys or model files. The current snippets do not show such advanced features.</li> </ul>"},{"location":"modules/core_app/","title":"Core App (<code>core</code>)","text":"<p>The <code>core</code> app is central to the Chakshu application's functionality. It orchestrates the main business logic, handles API requests for searching and processing Wikipedia articles, and interacts with other apps and external services.</p>"},{"location":"modules/core_app/#responsibilities","title":"Responsibilities","text":"<ul> <li>API Endpoint Handling: Defines and manages the primary API endpoints (<code>/search/</code>, <code>/select/</code>, <code>/process/</code>) through its views.</li> <li>Workflow Orchestration: Coordinates the flow of data and actions, such as receiving a search query, fetching results, allowing selection, and then delegating processing tasks to appropriate modules (scraper, captioner, WikiAPI).</li> <li>External API Interaction: Contains <code>wiki_api.py</code>, a dedicated client for communicating with the MediaWiki API to fetch structured data like page summaries.</li> <li>Data Formatting: Prepares and structures the JSON responses sent back to the client.</li> </ul>"},{"location":"modules/core_app/#key-files-and-components","title":"Key Files and Components","text":"<ul> <li> <p><code>views.py</code>:</p> <ul> <li><code>SearchResultsView</code>:<ul> <li>Handles <code>GET /api/search/</code>.</li> <li>Takes a search query <code>q</code>.</li> <li>Uses <code>googlesearch.search</code> to find Wikipedia articles.</li> <li>Employs <code>joblib.Parallel</code> and <code>joblib.delayed</code> for concurrent fetching of short descriptions via <code>scraper.views.get_short_description</code>.</li> <li>Formats and returns search results.</li> </ul> </li> <li><code>SelectLinkView</code>:<ul> <li>Handles <code>GET /api/select/</code>.</li> <li>Takes a Wikipedia article <code>link</code>.</li> <li>Returns a predefined list of processing options.</li> </ul> </li> <li><code>ProcessOptionView</code>:<ul> <li>Handles <code>GET /api/process/</code>.</li> <li>Takes a <code>link</code> and a numeric <code>option</code>.</li> <li>Acts as a dispatcher based on the <code>option</code>:<ul> <li>Option 1 (Short Description): Calls <code>scraper.views.get_short_description()</code>.</li> <li>Option 2 (Summary): Calls <code>mediawiki_api.get_page_summary()</code> (from <code>core.wiki_api.WikiAPI</code>).</li> <li>Option 3 (Full Content): Calls <code>scraper.views.get_full_content()</code>.</li> <li>Option 4 (Image Captions): Calls <code>captioner.views.fetch_and_process_images()</code>.</li> <li>Option 5 (Citations): Calls <code>scraper.views.get_citations()</code>.</li> </ul> </li> <li>Formats and returns the processed data.</li> </ul> </li> </ul> </li> <li> <p><code>urls.py</code>:</p> <ul> <li>Defines the URL patterns for the views mentioned above:<ul> <li><code>path(\"search/\", SearchResultsView.as_view(), name=\"search_results\")</code></li> <li><code>path(\"select/\", SelectLinkView.as_view(), name=\"select_link\")</code></li> <li><code>path(\"process/\", ProcessOptionView.as_view(), name=\"process_option\")</code></li> </ul> </li> <li>These URLs are included under the <code>/api/</code> namespace via <code>api/urls.py</code>.</li> </ul> </li> <li> <p><code>wiki_api.py</code>:</p> <ul> <li>Contains the <code>WikiAPI</code> class.</li> <li>This class is responsible for making requests to the MediaWiki API (e.g., <code>action=query&amp;format=json&amp;prop=extracts&amp;exintro=true&amp;explaintext=true</code> for summaries).</li> <li>The instance <code>mediawiki_api = WikiAPI(user_agent=\"Chakshu (chakshu@pec.edu.in)\")</code> is used by <code>ProcessOptionView</code>.</li> <li>Developer Note: The exact implementation details of <code>WikiAPI</code> (methods, error handling, request construction) should be examined by developers working on this module.</li> </ul> </li> <li> <p><code>models.py</code>:</p> <ul> <li>The provided file structure shows <code>models.py</code>. While the views don't explicitly interact with <code>core</code> models in the snippets, this file would define any database tables specific to the <code>core</code> app's needs (e.g., logging API requests, storing user preferences if features expand).</li> <li>The migrations history (<code>0001_initial</code> to <code>0008_delete_imagecaption_delete_wikipediapage</code>) indicates that models related to <code>WikipediaImageCaption</code>, <code>ImageCaption</code>, and <code>WikipediaPage</code> have existed in this app at various points. Developers should check the current <code>models.py</code> to understand the active data schema.</li> </ul> </li> <li> <p><code>hyperlinks.py</code>:</p> <ul> <li>The purpose and content of this file are unknown from the provided information. It might contain utility functions for manipulating or generating URLs.</li> </ul> </li> </ul>"},{"location":"modules/core_app/#interaction-with-other-modules","title":"Interaction with Other Modules","text":"<ul> <li><code>scraper</code> app: <code>core.views</code> calls functions in <code>scraper.views</code> to get scraped data like short descriptions, full content, and citations.</li> <li><code>captioner</code> app: <code>core.views</code> calls <code>captioner.views.fetch_and_process_images</code> for image captioning.</li> </ul>"},{"location":"modules/core_app/#development-considerations","title":"Development Considerations","text":"<ul> <li>Error Handling: Ensure robust error handling within views, especially for external calls (Google Search, WikiAPI, other apps).</li> <li>Performance: The use of <code>joblib</code> in <code>SearchResultsView</code> is a good practice. Evaluate other areas for potential performance bottlenecks, especially in <code>ProcessOptionView</code> when dealing with large pages or many images.</li> <li>Extensibility: Design views and services in a way that allows new processing options or data sources to be added with relative ease.</li> <li>Configuration: API keys or sensitive parameters for external services (like a more specific User-Agent for WikiAPI if needed) should ideally be managed via <code>settings.py</code> or <code>config.py</code> rather than being hardcoded.</li> </ul>"},{"location":"modules/scraper_app/","title":"Scraper App (<code>scraper</code>)","text":"<p>The <code>scraper</code> app is dedicated to fetching and parsing content from web pages, with a primary focus on Wikipedia articles.</p>"},{"location":"modules/scraper_app/#responsibilities","title":"Responsibilities","text":"<ul> <li>Content Fetching: Retrieving HTML content from given URLs.</li> <li>HTML Parsing: Extracting specific information from the HTML structure, such as:<ul> <li>Short descriptions</li> <li>Full page text</li> <li>Citations/references</li> <li>Image URLs</li> </ul> </li> <li>Data Cleaning: Potentially cleaning the extracted HTML or text content.</li> </ul>"},{"location":"modules/scraper_app/#key-files-and-components","title":"Key Files and Components","text":"<ul> <li> <p><code>views.py</code>:</p> <ul> <li>This file contains helper functions that are called by other parts of the application (primarily <code>core.views</code>) to perform scraping tasks. It does not seem to define direct API endpoints for external use in the main workflow but rather service functions.</li> <li><code>get_short_description(url)</code>:<ul> <li>Instantiates <code>WikipediaScraper</code>.</li> <li>Calls <code>scraper.fetch_wikipedia_content(url)</code> to get the page's <code>BeautifulSoup</code> object.</li> <li>Calls <code>scraper.fetch_short_description(soup)</code> to extract the short description.</li> </ul> </li> <li><code>get_citations(url)</code>:<ul> <li>Instantiates <code>WikipediaScraper</code>.</li> <li>Calls <code>scraper.main(url)</code> (presumably this method initializes scraping and populates scraper instance attributes like <code>citations</code>).</li> <li>Returns <code>scraper.citations</code>.</li> </ul> </li> <li><code>get_full_content(url)</code>:<ul> <li>Instantiates <code>WikipediaScraper</code>.</li> <li>Calls <code>scraper.main(url)</code> (or a similar method to get all content).</li> <li>Returns the full content.</li> </ul> </li> <li><code>get_image_urls(url)</code>:<ul> <li>Instantiates <code>WikipediaScraper</code>.</li> <li>Calls <code>scraper.main(url)</code>.</li> <li>Returns <code>scraper.images</code> (a list of image URLs).</li> </ul> </li> <li><code>WikiSummaryScraper</code> (Class-Based View):<ul> <li>This view (<code>GET</code> method) seems to be a standalone utility to fetch and return a summary by scraping paragraphs directly. It's not used in the main <code>core.views.ProcessOptionView</code> flow for summaries (which uses <code>WikiAPI</code>). It might be for internal testing or an alternative approach.</li> </ul> </li> </ul> </li> <li> <p><code>scraper.py</code> (Assumed Location):</p> <ul> <li>This file is expected to contain the <code>WikipediaScraper</code> class, which is the core of the scraping logic.</li> <li><code>WikipediaScraper</code> Class:<ul> <li><code>Workspace_wikipedia_content(url)</code>: Likely uses a library like <code>requests</code> to get the page HTML and <code>BeautifulSoup</code> to parse it.</li> <li><code>Workspace_short_description(soup)</code>: Implements logic to find and extract the short description element from the parsed HTML.</li> <li><code>main(url)</code>: A central method that likely orchestrates the fetching and parsing of various page elements (text, images, citations), storing them in instance attributes.</li> <li>Other methods for cleaning HTML (<code>clean_html_tags</code>), extracting specific sections, etc.</li> </ul> </li> <li>Developer Note: The implementation of <code>WikipediaScraper</code> is crucial. Developers should thoroughly understand its parsing logic, CSS selectors or XPath expressions used, and error handling for network issues or changes in Wikipedia's HTML structure.</li> </ul> </li> <li> <p><code>imageScrapper.py</code>:</p> <ul> <li>The purpose of this file is distinct from <code>scraper.py</code>. It might contain specialized logic for finding image URLs or processing images in a way that <code>scraper.py</code> doesn't cover, or it could be an alternative image scraping mechanism. Its interaction with <code>WikipediaScraper</code> or other parts of the app needs to be determined by examining its content.</li> </ul> </li> <li> <p><code>models.py</code>:</p> <ul> <li>If the scraper needs to store fetched data persistently (e.g., for caching raw HTML, parsed sections), models would be defined here. The directory structure includes it, but no specific models are detailed in the provided context.</li> </ul> </li> <li> <p><code>urls.py</code>:</p> <ul> <li>The file structure shows <code>scraper/urls.py</code>. While not explicitly detailed, if this app exposes any direct API endpoints (e.g., for testing scraper functionalities independently), they would be defined here and included in <code>api/urls.py</code>.</li> </ul> </li> </ul>"},{"location":"modules/scraper_app/#dependencies","title":"Dependencies","text":"<ul> <li><code>requests</code> (likely): For making HTTP requests.</li> <li><code>BeautifulSoup4</code> (likely): For parsing HTML.</li> </ul>"},{"location":"modules/scraper_app/#development-considerations","title":"Development Considerations","text":"<ul> <li>Robustness to HTML Changes: Web scraping is fragile and can break if the target website's structure changes. The selectors used in <code>WikipediaScraper</code> should be as resilient as possible. Consider using more stable identifiers if available (e.g., <code>id</code> attributes, specific classes).</li> <li>Rate Limiting/User-Agent: When scraping websites, it's important to be respectful. Implement proper User-Agent strings. For Wikipedia, the MediaWiki API (<code>core.wiki_api.py</code>) is generally preferred for structured data to avoid stressing raw page loads. The scraper might be used for content not easily available via the API.</li> <li>Error Handling: Implement comprehensive error handling for network errors, timeouts, and parsing errors (e.g., when expected HTML elements are not found).</li> <li>Maintainability: Keep scraping logic well-organized and documented, especially the parts that rely on specific HTML structures.</li> <li>Alternative Data Sources: For Wikipedia, always consider if the MediaWiki API can provide the required information more reliably than scraping. The current setup uses a mix, which is a good approach.</li> </ul>"}]}